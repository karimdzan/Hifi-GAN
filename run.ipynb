{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPZI2SWQaGyh",
        "outputId": "37410ec9-6118-461b-cc5a-40bcd39172a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Oct 27 09:17:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jf93GyLIAOZ"
      },
      "outputs": [],
      "source": [
        "!cd hw_asr && python3 train.py --config hw_asr/configs/conformer.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM1pMYRWU5ai",
        "outputId": "2d8f22b5-4023-4ad4-efc2-62948d8cc73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "Alphabet determined to be of regular style.\n",
            "CTC blank char '' not found, appending to end.\n",
            "1 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "13243 (46.4%) records are longer then 200 characters. Excluding them.\n",
            "Filtered 13243(46.4%) records  from dataset\n",
            "17 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "48340 (46.5%) records are longer then 200 characters. Excluding them.\n",
            "Filtered 48340(46.5%) records  from dataset\n",
            "37 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "56078 (37.7%) records are longer then 200 characters. Excluding them.\n",
            "Filtered 56078(37.7%) records  from dataset\n",
            "92 (3.5%) records are longer then 20.0 seconds. Excluding them.\n",
            "334 (12.7%) records are longer then 200 characters. Excluding them.\n",
            "Filtered 335(12.8%) records  from dataset\n",
            "46 (1.6%) records are longer then 20.0 seconds. Excluding them.\n",
            "240 (8.2%) records are longer then 200 characters. Excluding them.\n",
            "Filtered 240(8.2%) records  from dataset\n",
            "Conformer(\n",
            "  (layers): ModuleList(\n",
            "    (0-11): 12 x ConformerBlock(\n",
            "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff1): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (2): Swish()\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "          (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (5): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (attn): MultiHeadedSelfAttentionModule(\n",
            "        (positional_encoding): PositionalEncoding()\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention): RelativeMultiHeadAttention(\n",
            "          (query_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (key_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (value_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (pos_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (conv): ConformerConvModule(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Rearrange('b n c -> b c n')\n",
            "          (2): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
            "          (3): GLU()\n",
            "          (4): DepthWiseConv1d(\n",
            "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), groups=256)\n",
            "          )\n",
            "          (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (6): Swish()\n",
            "          (7): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "          (8): Rearrange('b c n -> b n c')\n",
            "          (9): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ff2): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (2): Swish()\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "          (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (5): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_subsample): Conv2dSubsampling(\n",
            "    (sequential): Sequential(\n",
            "      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (input_proj): Sequential(\n",
            "    (0): Linear(in_features=3968, out_features=128, bias=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=28, bias=False)\n",
            ")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarimdzan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/hw_asr/wandb/run-20231028_152315-wqc6wfns\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpert-yogurt-65\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/karimdzan/asr_project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/karimdzan/asr_project/runs/wqc6wfns\u001b[0m\n",
            "Loading checkpoint: /content/hw_asr/saved/models/conformer/fine/model_best_fine.pth ...\n",
            "[0.000489465020885939]\n",
            "27555\n",
            "Checkpoint loaded. Resume training from epoch 56\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 56 [0/500 (0%)] Loss: 0.365216\n",
            "train:  10% 50/500 [00:43<06:03,  1.24it/s]Train Epoch: 56 [50/500 (10%)] Loss: 0.390202\n",
            "train:  20% 100/500 [01:23<05:23,  1.24it/s]Train Epoch: 56 [100/500 (20%)] Loss: 0.406048\n",
            "train:  30% 150/500 [02:03<04:33,  1.28it/s]Train Epoch: 56 [150/500 (30%)] Loss: 0.387133\n",
            "train:  40% 200/500 [02:43<04:03,  1.23it/s]Train Epoch: 56 [200/500 (40%)] Loss: 0.433085\n",
            "train:  50% 250/500 [03:24<03:16,  1.27it/s]Train Epoch: 56 [250/500 (50%)] Loss: 0.465710\n",
            "train:  60% 300/500 [04:04<02:39,  1.26it/s]Train Epoch: 56 [300/500 (60%)] Loss: 0.359865\n",
            "train:  70% 350/500 [04:45<01:58,  1.27it/s]Train Epoch: 56 [350/500 (70%)] Loss: 0.442527\n",
            "train:  80% 400/500 [05:26<01:19,  1.25it/s]Train Epoch: 56 [400/500 (80%)] Loss: 0.391196\n",
            "train:  90% 450/500 [06:07<00:38,  1.28it/s]Train Epoch: 56 [450/500 (90%)] Loss: 0.481397\n",
            "train: 100% 500/500 [06:48<00:00,  1.16it/s]Train Epoch: 56 [500/500 (100%)] Loss: 0.418339\n",
            "train: 100% 500/500 [06:50<00:00,  1.22it/s]\n",
            "val: 100% 78/78 [00:15<00:00,  5.19it/s]\n",
            "    epoch          : 56\n",
            "    loss           : 0.4391786330938339\n",
            "    grad norm      : 1.171580195426941\n",
            "    WER (argmax)   : 0.3409070767492758\n",
            "    CER (argmax)   : 0.11713804806544921\n",
            "    val_loss       : 0.631009898124597\n",
            "    val_WER (argmax): 0.4220840008043992\n",
            "    val_CER (argmax): 0.16017001127546812\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 57 [0/500 (0%)] Loss: 0.406746\n",
            "train:  10% 50/500 [00:40<05:59,  1.25it/s]Train Epoch: 57 [50/500 (10%)] Loss: 0.434079\n",
            "train:  20% 100/500 [01:21<05:32,  1.20it/s]Train Epoch: 57 [100/500 (20%)] Loss: 0.438351\n",
            "train:  30% 150/500 [02:03<04:43,  1.24it/s]Train Epoch: 57 [150/500 (30%)] Loss: 0.406479\n",
            "train:  40% 200/500 [02:44<04:01,  1.24it/s]Train Epoch: 57 [200/500 (40%)] Loss: 0.406964\n",
            "train:  50% 250/500 [03:25<03:28,  1.20it/s]Train Epoch: 57 [250/500 (50%)] Loss: 0.428311\n",
            "train:  60% 300/500 [04:06<02:38,  1.26it/s]Train Epoch: 57 [300/500 (60%)] Loss: 0.459940\n",
            "train:  70% 350/500 [04:47<02:03,  1.22it/s]Train Epoch: 57 [350/500 (70%)] Loss: 0.433430\n",
            "train:  80% 400/500 [05:29<01:18,  1.27it/s]Train Epoch: 57 [400/500 (80%)] Loss: 0.401229\n",
            "train:  90% 450/500 [06:11<00:39,  1.25it/s]Train Epoch: 57 [450/500 (90%)] Loss: 0.387689\n",
            "train: 100% 500/500 [06:52<00:00,  1.22it/s]Train Epoch: 57 [500/500 (100%)] Loss: 0.376013\n",
            "train: 100% 500/500 [06:54<00:00,  1.21it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.39it/s]\n",
            "    epoch          : 57\n",
            "    loss           : 0.42559216856956483\n",
            "    grad norm      : 1.0720450782775879\n",
            "    WER (argmax)   : 0.33383163909456826\n",
            "    CER (argmax)   : 0.11481969613880556\n",
            "    val_loss       : 0.6265568473400214\n",
            "    val_WER (argmax): 0.4242273110077626\n",
            "    val_CER (argmax): 0.16127211726761648\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 58 [0/500 (0%)] Loss: 0.444020\n",
            "train:  10% 50/500 [00:41<05:59,  1.25it/s]Train Epoch: 58 [50/500 (10%)] Loss: 0.366754\n",
            "train:  20% 100/500 [01:23<05:40,  1.17it/s]Train Epoch: 58 [100/500 (20%)] Loss: 0.459841\n",
            "train:  30% 150/500 [02:05<04:44,  1.23it/s]Train Epoch: 58 [150/500 (30%)] Loss: 0.488719\n",
            "train:  40% 200/500 [02:47<04:04,  1.23it/s]Train Epoch: 58 [200/500 (40%)] Loss: 0.435821\n",
            "train:  50% 250/500 [03:29<03:25,  1.22it/s]Train Epoch: 58 [250/500 (50%)] Loss: 0.525182\n",
            "train:  60% 300/500 [04:11<02:45,  1.21it/s]Train Epoch: 58 [300/500 (60%)] Loss: 0.391041\n",
            "train:  70% 350/500 [04:53<02:01,  1.24it/s]Train Epoch: 58 [350/500 (70%)] Loss: 0.418882\n",
            "train:  80% 400/500 [05:36<01:25,  1.17it/s]Train Epoch: 58 [400/500 (80%)] Loss: 0.562262\n",
            "train:  90% 450/500 [06:18<00:40,  1.22it/s]Train Epoch: 58 [450/500 (90%)] Loss: 0.357915\n",
            "train: 100% 500/500 [07:00<00:00,  1.22it/s]Train Epoch: 58 [500/500 (100%)] Loss: 0.432990\n",
            "train: 100% 500/500 [07:02<00:00,  1.18it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.34it/s]\n",
            "    epoch          : 58\n",
            "    loss           : 0.43014820635318757\n",
            "    grad norm      : 1.0750295066833495\n",
            "    WER (argmax)   : 0.3366624320530404\n",
            "    CER (argmax)   : 0.11632158078091927\n",
            "    val_loss       : 0.644862800072401\n",
            "    val_WER (argmax): 0.4372973698884964\n",
            "    val_CER (argmax): 0.1663010108066701\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 59 [0/500 (0%)] Loss: 0.448448\n",
            "train:  10% 50/500 [00:42<06:17,  1.19it/s]Train Epoch: 59 [50/500 (10%)] Loss: 0.459194\n",
            "train:  20% 100/500 [01:24<05:23,  1.23it/s]Train Epoch: 59 [100/500 (20%)] Loss: 0.452894\n",
            "train:  30% 150/500 [02:07<04:49,  1.21it/s]Train Epoch: 59 [150/500 (30%)] Loss: 0.539873\n",
            "train:  40% 200/500 [02:50<04:07,  1.21it/s]Train Epoch: 59 [200/500 (40%)] Loss: 0.495248\n",
            "train:  50% 250/500 [03:33<03:31,  1.18it/s]Train Epoch: 59 [250/500 (50%)] Loss: 0.407468\n",
            "train:  60% 300/500 [04:16<02:43,  1.23it/s]Train Epoch: 59 [300/500 (60%)] Loss: 0.386803\n",
            "train:  70% 350/500 [04:58<02:05,  1.20it/s]Train Epoch: 59 [350/500 (70%)] Loss: 0.460100\n",
            "train:  80% 400/500 [05:41<01:24,  1.18it/s]Train Epoch: 59 [400/500 (80%)] Loss: 0.390693\n",
            "train:  90% 450/500 [06:24<00:41,  1.20it/s]Train Epoch: 59 [450/500 (90%)] Loss: 0.573442\n",
            "train: 100% 500/500 [07:07<00:00,  1.18it/s]Train Epoch: 59 [500/500 (100%)] Loss: 0.467382\n",
            "train: 100% 500/500 [07:08<00:00,  1.17it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.35it/s]\n",
            "    epoch          : 59\n",
            "    loss           : 0.4378119844198227\n",
            "    grad norm      : 1.0939616668224335\n",
            "    WER (argmax)   : 0.3360666206891542\n",
            "    CER (argmax)   : 0.11632450412421987\n",
            "    val_loss       : 0.6482921021106915\n",
            "    val_WER (argmax): 0.4203136478824419\n",
            "    val_CER (argmax): 0.1612585361501189\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 60 [0/500 (0%)] Loss: 0.429812\n",
            "train:  10% 50/500 [00:42<06:21,  1.18it/s]Train Epoch: 60 [50/500 (10%)] Loss: 0.432573\n",
            "train:  20% 100/500 [01:26<05:35,  1.19it/s]Train Epoch: 60 [100/500 (20%)] Loss: 0.430435\n",
            "train:  30% 150/500 [02:09<04:51,  1.20it/s]Train Epoch: 60 [150/500 (30%)] Loss: 0.375907\n",
            "train:  40% 200/500 [02:52<04:08,  1.21it/s]Train Epoch: 60 [200/500 (40%)] Loss: 0.470345\n",
            "train:  50% 250/500 [03:36<03:31,  1.18it/s]Train Epoch: 60 [250/500 (50%)] Loss: 0.401810\n",
            "train:  60% 300/500 [04:20<02:53,  1.15it/s]Train Epoch: 60 [300/500 (60%)] Loss: 0.410972\n",
            "train:  70% 350/500 [05:03<02:06,  1.19it/s]Train Epoch: 60 [350/500 (70%)] Loss: 0.400170\n",
            "train:  80% 400/500 [05:47<01:26,  1.16it/s]Train Epoch: 60 [400/500 (80%)] Loss: 0.404325\n",
            "train:  90% 450/500 [06:31<00:42,  1.17it/s]Train Epoch: 60 [450/500 (90%)] Loss: 0.492667\n",
            "train: 100% 500/500 [07:15<00:00,  1.14it/s]Train Epoch: 60 [500/500 (100%)] Loss: 0.476743\n",
            "train: 100% 500/500 [07:17<00:00,  1.14it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.23it/s]\n",
            "    epoch          : 60\n",
            "    loss           : 0.4350605046749115\n",
            "    grad norm      : 1.0862248122692109\n",
            "    WER (argmax)   : 0.3339271188310983\n",
            "    CER (argmax)   : 0.11543810779191223\n",
            "    val_loss       : 0.6301647783854069\n",
            "    val_WER (argmax): 0.4203766670336968\n",
            "    val_CER (argmax): 0.1612669625768124\n",
            "Saving checkpoint: saved/models/conformer/1028_152312/checkpoint-epoch60.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 61 [0/500 (0%)] Loss: 0.471150\n",
            "train:  10% 50/500 [00:43<06:15,  1.20it/s]Train Epoch: 61 [50/500 (10%)] Loss: 0.401077\n",
            "train:  20% 100/500 [01:28<05:38,  1.18it/s]Train Epoch: 61 [100/500 (20%)] Loss: 0.395818\n",
            "train:  30% 150/500 [02:13<04:57,  1.17it/s]Train Epoch: 61 [150/500 (30%)] Loss: 0.389051\n",
            "train:  40% 200/500 [02:57<04:27,  1.12it/s]Train Epoch: 61 [200/500 (40%)] Loss: 0.388894\n",
            "train:  50% 250/500 [03:42<03:49,  1.09it/s]Train Epoch: 61 [250/500 (50%)] Loss: 0.534760\n",
            "train:  60% 300/500 [04:26<02:50,  1.17it/s]Train Epoch: 61 [300/500 (60%)] Loss: 0.427311\n",
            "train:  70% 350/500 [05:11<02:10,  1.15it/s]Train Epoch: 61 [350/500 (70%)] Loss: 0.443875\n",
            "train:  80% 400/500 [05:55<01:24,  1.18it/s]Train Epoch: 61 [400/500 (80%)] Loss: 0.399950\n",
            "train:  90% 450/500 [06:40<00:43,  1.15it/s]Train Epoch: 61 [450/500 (90%)] Loss: 0.388167\n",
            "train: 100% 500/500 [07:24<00:00,  1.15it/s]Train Epoch: 61 [500/500 (100%)] Loss: 0.438903\n",
            "train: 100% 500/500 [07:26<00:00,  1.12it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.33it/s]\n",
            "    epoch          : 61\n",
            "    loss           : 0.41258047223091127\n",
            "    grad norm      : 1.0596095466613769\n",
            "    WER (argmax)   : 0.3249909866985666\n",
            "    CER (argmax)   : 0.11117340562294334\n",
            "    val_loss       : 0.6219366490840912\n",
            "    val_WER (argmax): 0.4143014206258155\n",
            "    val_CER (argmax): 0.15938207042582664\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 62 [0/500 (0%)] Loss: 0.461023\n",
            "train:  10% 50/500 [00:45<06:27,  1.16it/s]Train Epoch: 62 [50/500 (10%)] Loss: 0.425315\n",
            "train:  20% 100/500 [01:30<05:59,  1.11it/s]Train Epoch: 62 [100/500 (20%)] Loss: 0.449348\n",
            "train:  30% 150/500 [02:16<05:16,  1.11it/s]Train Epoch: 62 [150/500 (30%)] Loss: 0.409120\n",
            "train:  40% 200/500 [03:01<04:21,  1.15it/s]Train Epoch: 62 [200/500 (40%)] Loss: 0.416404\n",
            "train:  50% 250/500 [03:46<03:44,  1.11it/s]Train Epoch: 62 [250/500 (50%)] Loss: 0.402577\n",
            "train:  60% 300/500 [04:32<03:06,  1.07it/s]Train Epoch: 62 [300/500 (60%)] Loss: 0.385586\n",
            "train:  70% 350/500 [05:18<02:15,  1.11it/s]Train Epoch: 62 [350/500 (70%)] Loss: 0.371269\n",
            "train:  80% 400/500 [06:04<01:30,  1.10it/s]Train Epoch: 62 [400/500 (80%)] Loss: 0.459788\n",
            "train:  90% 450/500 [06:50<00:44,  1.11it/s]Train Epoch: 62 [450/500 (90%)] Loss: 0.381839\n",
            "train: 100% 500/500 [07:37<00:00,  1.12it/s]Train Epoch: 62 [500/500 (100%)] Loss: 0.379131\n",
            "train: 100% 500/500 [07:38<00:00,  1.09it/s]\n",
            "val: 100% 78/78 [00:15<00:00,  5.14it/s]\n",
            "    epoch          : 62\n",
            "    loss           : 0.4046792280673981\n",
            "    grad norm      : 1.1012950015068055\n",
            "    WER (argmax)   : 0.31736436212272334\n",
            "    CER (argmax)   : 0.10922957364376304\n",
            "    val_loss       : 0.5957288933105958\n",
            "    val_WER (argmax): 0.40685332758090875\n",
            "    val_CER (argmax): 0.15401775228961806\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 63 [0/500 (0%)] Loss: 0.380126\n",
            "train:  10% 50/500 [00:47<06:42,  1.12it/s]Train Epoch: 63 [50/500 (10%)] Loss: 0.367927\n",
            "train:  20% 100/500 [01:33<06:07,  1.09it/s]Train Epoch: 63 [100/500 (20%)] Loss: 0.438760\n",
            "train:  30% 150/500 [02:20<05:32,  1.05it/s]Train Epoch: 63 [150/500 (30%)] Loss: 0.463224\n",
            "train:  40% 200/500 [03:06<04:36,  1.08it/s]Train Epoch: 63 [200/500 (40%)] Loss: 0.415122\n",
            "train:  50% 250/500 [03:52<03:46,  1.11it/s]Train Epoch: 63 [250/500 (50%)] Loss: 0.418224\n",
            "train:  60% 300/500 [04:38<03:01,  1.10it/s]Train Epoch: 63 [300/500 (60%)] Loss: 0.364498\n",
            "train:  70% 350/500 [05:25<02:14,  1.12it/s]Train Epoch: 63 [350/500 (70%)] Loss: 0.397422\n",
            "train:  80% 400/500 [06:12<01:29,  1.12it/s]Train Epoch: 63 [400/500 (80%)] Loss: 0.439618\n",
            "train:  90% 450/500 [06:58<00:45,  1.10it/s]Train Epoch: 63 [450/500 (90%)] Loss: 0.390344\n",
            "train: 100% 500/500 [07:45<00:00,  1.11it/s]Train Epoch: 63 [500/500 (100%)] Loss: 0.407972\n",
            "train: 100% 500/500 [07:46<00:00,  1.07it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.45it/s]\n",
            "    epoch          : 63\n",
            "    loss           : 0.40129449307918547\n",
            "    grad norm      : 1.0674673295021058\n",
            "    WER (argmax)   : 0.31304625864547037\n",
            "    CER (argmax)   : 0.10714971679022608\n",
            "    val_loss       : 0.599513502075122\n",
            "    val_WER (argmax): 0.4029835238132979\n",
            "    val_CER (argmax): 0.15309574713267254\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 64 [0/500 (0%)] Loss: 0.358121\n",
            "train:  10% 50/500 [00:46<06:49,  1.10it/s]Train Epoch: 64 [50/500 (10%)] Loss: 0.482013\n",
            "train:  20% 100/500 [01:33<06:12,  1.07it/s]Train Epoch: 64 [100/500 (20%)] Loss: 0.393778\n",
            "train:  30% 150/500 [02:20<05:36,  1.04it/s]Train Epoch: 64 [150/500 (30%)] Loss: 0.411426\n",
            "train:  40% 200/500 [03:07<04:36,  1.09it/s]Train Epoch: 64 [200/500 (40%)] Loss: 0.373711\n",
            "train:  50% 250/500 [03:54<03:54,  1.07it/s]Train Epoch: 64 [250/500 (50%)] Loss: 0.434008\n",
            "train:  60% 300/500 [04:41<03:04,  1.08it/s]Train Epoch: 64 [300/500 (60%)] Loss: 0.343787\n",
            "train:  70% 350/500 [05:28<02:19,  1.08it/s]Train Epoch: 64 [350/500 (70%)] Loss: 0.529126\n",
            "train:  80% 400/500 [06:16<01:33,  1.07it/s]Train Epoch: 64 [400/500 (80%)] Loss: 0.393153\n",
            "train:  90% 450/500 [07:03<00:46,  1.08it/s]Train Epoch: 64 [450/500 (90%)] Loss: 0.388808\n",
            "train: 100% 500/500 [07:50<00:00,  1.08it/s]Train Epoch: 64 [500/500 (100%)] Loss: 0.409049\n",
            "train: 100% 500/500 [07:52<00:00,  1.06it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.28it/s]\n",
            "    epoch          : 64\n",
            "    loss           : 0.4011470425128937\n",
            "    grad norm      : 1.020639853477478\n",
            "    WER (argmax)   : 0.31457149532355255\n",
            "    CER (argmax)   : 0.10824418965185105\n",
            "    val_loss       : 0.6069221160350702\n",
            "    val_WER (argmax): 0.39883038715413166\n",
            "    val_CER (argmax): 0.15221946082541793\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 65 [0/500 (0%)] Loss: 0.363887\n",
            "train:  10% 50/500 [00:48<07:00,  1.07it/s]Train Epoch: 65 [50/500 (10%)] Loss: 0.449167\n",
            "train:  20% 100/500 [01:36<06:12,  1.07it/s]Train Epoch: 65 [100/500 (20%)] Loss: 0.353714\n",
            "train:  30% 150/500 [02:24<05:45,  1.01it/s]Train Epoch: 65 [150/500 (30%)] Loss: 0.375995\n",
            "train:  40% 200/500 [03:12<04:45,  1.05it/s]Train Epoch: 65 [200/500 (40%)] Loss: 0.396188\n",
            "train:  50% 250/500 [04:00<03:53,  1.07it/s]Train Epoch: 65 [250/500 (50%)] Loss: 0.355228\n",
            "train:  60% 300/500 [04:49<03:11,  1.04it/s]Train Epoch: 65 [300/500 (60%)] Loss: 0.366626\n",
            "train:  70% 350/500 [05:37<02:21,  1.06it/s]Train Epoch: 65 [350/500 (70%)] Loss: 0.403803\n",
            "train:  80% 400/500 [06:26<01:33,  1.07it/s]Train Epoch: 65 [400/500 (80%)] Loss: 0.469956\n",
            "train:  90% 450/500 [07:15<00:48,  1.03it/s]Train Epoch: 65 [450/500 (90%)] Loss: 0.484190\n",
            "train: 100% 500/500 [08:04<00:00,  1.03it/s]Train Epoch: 65 [500/500 (100%)] Loss: 0.460738\n",
            "train: 100% 500/500 [08:06<00:00,  1.03it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.37it/s]\n",
            "    epoch          : 65\n",
            "    loss           : 0.4104396003484726\n",
            "    grad norm      : 1.1010139536857606\n",
            "    WER (argmax)   : 0.3170475378097644\n",
            "    CER (argmax)   : 0.10891690016533168\n",
            "    val_loss       : 0.607708528255805\n",
            "    val_WER (argmax): 0.4117544480775004\n",
            "    val_CER (argmax): 0.15731820063491073\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 66 [0/500 (0%)] Loss: 0.419904\n",
            "train:  10% 50/500 [00:49<07:24,  1.01it/s]Train Epoch: 66 [50/500 (10%)] Loss: 0.403974\n",
            "train:  20% 100/500 [01:39<06:28,  1.03it/s]Train Epoch: 66 [100/500 (20%)] Loss: 0.399178\n",
            "train:  30% 150/500 [02:29<05:53,  1.01s/it]Train Epoch: 66 [150/500 (30%)] Loss: 0.334585\n",
            "train:  40% 200/500 [03:18<04:45,  1.05it/s]Train Epoch: 66 [200/500 (40%)] Loss: 0.429565\n",
            "train:  50% 250/500 [04:07<04:02,  1.03it/s]Train Epoch: 66 [250/500 (50%)] Loss: 0.458735\n",
            "train:  60% 300/500 [04:56<03:10,  1.05it/s]Train Epoch: 66 [300/500 (60%)] Loss: 0.452111\n",
            "train:  70% 350/500 [05:46<02:26,  1.02it/s]Train Epoch: 66 [350/500 (70%)] Loss: 0.431884\n",
            "train:  80% 400/500 [06:35<01:40,  1.00s/it]Train Epoch: 66 [400/500 (80%)] Loss: 0.474924\n",
            "train:  90% 450/500 [07:25<00:48,  1.04it/s]Train Epoch: 66 [450/500 (90%)] Loss: 0.377520\n",
            "train: 100% 500/500 [08:15<00:00,  1.03it/s]Train Epoch: 66 [500/500 (100%)] Loss: 0.492167\n",
            "train: 100% 500/500 [08:17<00:00,  1.00it/s]\n",
            "val: 100% 78/78 [00:14<00:00,  5.39it/s]\n",
            "    epoch          : 66\n",
            "    loss           : 0.3927312117815018\n",
            "    grad norm      : 1.0281833016872406\n",
            "    WER (argmax)   : 0.30638828970240295\n",
            "    CER (argmax)   : 0.10579957224365456\n",
            "    val_loss       : 0.5961395272841821\n",
            "    val_WER (argmax): 0.4013480997508539\n",
            "    val_CER (argmax): 0.15403969113613733\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 67 [0/500 (0%)] Loss: 0.297053\n",
            "train:  10% 50/500 [00:49<07:18,  1.03it/s]Train Epoch: 67 [50/500 (10%)] Loss: 0.377740\n",
            "train:  20% 100/500 [01:39<06:41,  1.00s/it]Train Epoch: 67 [100/500 (20%)] Loss: 0.344981\n",
            "train:  30% 150/500 [02:30<05:46,  1.01it/s]Train Epoch: 67 [150/500 (30%)] Loss: 0.414422\n",
            "train:  40% 200/500 [03:20<04:54,  1.02it/s]Train Epoch: 67 [200/500 (40%)] Loss: 0.367657\n",
            "train:  50% 250/500 [04:11<04:08,  1.01it/s]Train Epoch: 67 [250/500 (50%)] Loss: 0.412481\n",
            "train:  60% 300/500 [05:01<03:13,  1.03it/s]Train Epoch: 67 [300/500 (60%)] Loss: 0.375252\n",
            "train:  70% 350/500 [05:52<02:27,  1.02it/s]Train Epoch: 67 [350/500 (70%)] Loss: 0.396254\n",
            "train:  80% 400/500 [06:43<01:39,  1.01it/s]Train Epoch: 67 [400/500 (80%)] Loss: 0.353623\n",
            "train:  90% 450/500 [07:33<00:49,  1.01it/s]Train Epoch: 67 [450/500 (90%)] Loss: 0.426332\n",
            "train: 100% 500/500 [08:24<00:00,  1.00it/s]Train Epoch: 67 [500/500 (100%)] Loss: 0.398012\n",
            "train: 100% 500/500 [08:26<00:00,  1.01s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.23it/s]\n",
            "    epoch          : 67\n",
            "    loss           : 0.39762880861759187\n",
            "    grad norm      : 1.122627956867218\n",
            "    WER (argmax)   : 0.3082289523390443\n",
            "    CER (argmax)   : 0.10582337111251638\n",
            "    val_loss       : 0.599981461197902\n",
            "    val_WER (argmax): 0.3958087638991854\n",
            "    val_CER (argmax): 0.1501438724501209\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 68 [0/500 (0%)] Loss: 0.408702\n",
            "train:  10% 50/500 [00:51<07:47,  1.04s/it]Train Epoch: 68 [50/500 (10%)] Loss: 0.380215\n",
            "train:  20% 100/500 [01:42<06:43,  1.01s/it]Train Epoch: 68 [100/500 (20%)] Loss: 0.404874\n",
            "train:  30% 150/500 [02:34<05:49,  1.00it/s]Train Epoch: 68 [150/500 (30%)] Loss: 0.447014\n",
            "train:  40% 200/500 [03:26<05:02,  1.01s/it]Train Epoch: 68 [200/500 (40%)] Loss: 0.345395\n",
            "train:  50% 250/500 [04:17<04:12,  1.01s/it]Train Epoch: 68 [250/500 (50%)] Loss: 0.396044\n",
            "train:  60% 300/500 [05:08<03:21,  1.01s/it]Train Epoch: 68 [300/500 (60%)] Loss: 0.429138\n",
            "train:  70% 350/500 [06:00<02:35,  1.04s/it]Train Epoch: 68 [350/500 (70%)] Loss: 0.350412\n",
            "train:  80% 400/500 [06:52<01:40,  1.01s/it]Train Epoch: 68 [400/500 (80%)] Loss: 0.363661\n",
            "train:  90% 450/500 [07:44<00:50,  1.00s/it]Train Epoch: 68 [450/500 (90%)] Loss: 0.375241\n",
            "train: 100% 500/500 [08:36<00:00,  1.01s/it]Train Epoch: 68 [500/500 (100%)] Loss: 0.343324\n",
            "train: 100% 500/500 [08:38<00:00,  1.04s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.23it/s]\n",
            "    epoch          : 68\n",
            "    loss           : 0.39390028059482574\n",
            "    grad norm      : 1.0628160393238069\n",
            "    WER (argmax)   : 0.3028799221212894\n",
            "    CER (argmax)   : 0.10470168374488617\n",
            "    val_loss       : 0.5809432447720797\n",
            "    val_WER (argmax): 0.39340567773297364\n",
            "    val_CER (argmax): 0.14977917227204987\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 69 [0/500 (0%)] Loss: 0.413895\n",
            "train:  10% 50/500 [00:52<07:56,  1.06s/it]Train Epoch: 69 [50/500 (10%)] Loss: 0.446579\n",
            "train:  20% 100/500 [01:45<06:58,  1.05s/it]Train Epoch: 69 [100/500 (20%)] Loss: 0.341903\n",
            "train:  30% 150/500 [02:38<06:06,  1.05s/it]Train Epoch: 69 [150/500 (30%)] Loss: 0.351176\n",
            "train:  40% 200/500 [03:30<05:23,  1.08s/it]Train Epoch: 69 [200/500 (40%)] Loss: 0.383205\n",
            "train:  50% 250/500 [04:24<04:17,  1.03s/it]Train Epoch: 69 [250/500 (50%)] Loss: 0.395303\n",
            "train:  60% 300/500 [05:16<03:29,  1.05s/it]Train Epoch: 69 [300/500 (60%)] Loss: 0.361982\n",
            "train:  70% 350/500 [06:10<02:33,  1.03s/it]Train Epoch: 69 [350/500 (70%)] Loss: 0.369727\n",
            "train:  80% 400/500 [07:03<01:45,  1.06s/it]Train Epoch: 69 [400/500 (80%)] Loss: 0.365602\n",
            "train:  90% 450/500 [07:56<00:51,  1.04s/it]Train Epoch: 69 [450/500 (90%)] Loss: 0.372402\n",
            "train: 100% 500/500 [08:49<00:00,  1.06s/it]Train Epoch: 69 [500/500 (100%)] Loss: 0.351379\n",
            "train: 100% 500/500 [08:51<00:00,  1.06s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.33it/s]\n",
            "    epoch          : 69\n",
            "    loss           : 0.38539029002189634\n",
            "    grad norm      : 1.0461673545837402\n",
            "    WER (argmax)   : 0.2991931961353793\n",
            "    CER (argmax)   : 0.10289171843274149\n",
            "    val_loss       : 0.5757493319419714\n",
            "    val_WER (argmax): 0.39558943001485836\n",
            "    val_CER (argmax): 0.15040644108995022\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 70 [0/500 (0%)] Loss: 0.355920\n",
            "train:  10% 50/500 [00:53<07:40,  1.02s/it]Train Epoch: 70 [50/500 (10%)] Loss: 0.367940\n",
            "train:  20% 100/500 [01:47<07:15,  1.09s/it]Train Epoch: 70 [100/500 (20%)] Loss: 0.424979\n",
            "train:  30% 150/500 [02:40<06:13,  1.07s/it]Train Epoch: 70 [150/500 (30%)] Loss: 0.355879\n",
            "train:  40% 200/500 [03:34<05:20,  1.07s/it]Train Epoch: 70 [200/500 (40%)] Loss: 0.365669\n",
            "train:  50% 250/500 [04:27<04:16,  1.03s/it]Train Epoch: 70 [250/500 (50%)] Loss: 0.427158\n",
            "train:  60% 300/500 [05:21<03:31,  1.06s/it]Train Epoch: 70 [300/500 (60%)] Loss: 0.391443\n",
            "train:  70% 350/500 [06:14<02:35,  1.04s/it]Train Epoch: 70 [350/500 (70%)] Loss: 0.423827\n",
            "train:  80% 400/500 [07:08<01:43,  1.04s/it]Train Epoch: 70 [400/500 (80%)] Loss: 0.423734\n",
            "train:  90% 450/500 [08:01<00:52,  1.04s/it]Train Epoch: 70 [450/500 (90%)] Loss: 0.346100\n",
            "train: 100% 500/500 [08:55<00:00,  1.05s/it]Train Epoch: 70 [500/500 (100%)] Loss: 0.321615\n",
            "train: 100% 500/500 [08:57<00:00,  1.07s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.35it/s]\n",
            "    epoch          : 70\n",
            "    loss           : 0.3826363182067871\n",
            "    grad norm      : 1.020551357269287\n",
            "    WER (argmax)   : 0.30484302387829976\n",
            "    CER (argmax)   : 0.10323791049932521\n",
            "    val_loss       : 0.580955473276285\n",
            "    val_WER (argmax): 0.38782218429061693\n",
            "    val_CER (argmax): 0.14736671390415612\n",
            "Saving checkpoint: saved/models/conformer/1028_152312/checkpoint-epoch70.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 71 [0/500 (0%)] Loss: 0.397281\n",
            "train:  10% 50/500 [00:54<08:24,  1.12s/it]Train Epoch: 71 [50/500 (10%)] Loss: 0.417286\n",
            "train:  20% 100/500 [01:47<06:55,  1.04s/it]Train Epoch: 71 [100/500 (20%)] Loss: 0.385096\n",
            "train:  30% 150/500 [02:41<05:59,  1.03s/it]Train Epoch: 71 [150/500 (30%)] Loss: 0.351386\n",
            "train:  40% 200/500 [03:36<05:20,  1.07s/it]Train Epoch: 71 [200/500 (40%)] Loss: 0.334601\n",
            "train:  50% 250/500 [04:31<04:23,  1.06s/it]Train Epoch: 71 [250/500 (50%)] Loss: 0.355285\n",
            "train:  60% 300/500 [05:25<03:31,  1.06s/it]Train Epoch: 71 [300/500 (60%)] Loss: 0.428593\n",
            "train:  70% 350/500 [06:19<02:36,  1.05s/it]Train Epoch: 71 [350/500 (70%)] Loss: 0.341463\n",
            "train:  80% 400/500 [07:14<01:47,  1.07s/it]Train Epoch: 71 [400/500 (80%)] Loss: 0.360806\n",
            "train:  90% 450/500 [08:09<00:52,  1.04s/it]Train Epoch: 71 [450/500 (90%)] Loss: 0.382652\n",
            "train: 100% 500/500 [09:04<00:00,  1.08s/it]Train Epoch: 71 [500/500 (100%)] Loss: 0.344313\n",
            "train: 100% 500/500 [09:06<00:00,  1.09s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.42it/s]\n",
            "    epoch          : 71\n",
            "    loss           : 0.36370857000350953\n",
            "    grad norm      : 0.9684919369220734\n",
            "    WER (argmax)   : 0.29188457965049\n",
            "    CER (argmax)   : 0.09960541668546034\n",
            "    val_loss       : 0.5785820533831915\n",
            "    val_WER (argmax): 0.391786703993481\n",
            "    val_CER (argmax): 0.14991966845625587\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 72 [0/500 (0%)] Loss: 0.343604\n",
            "train:  10% 50/500 [00:54<08:09,  1.09s/it]Train Epoch: 72 [50/500 (10%)] Loss: 0.327801\n",
            "train:  20% 100/500 [01:49<07:01,  1.05s/it]Train Epoch: 72 [100/500 (20%)] Loss: 0.325929\n",
            "train:  30% 150/500 [02:45<06:18,  1.08s/it]Train Epoch: 72 [150/500 (30%)] Loss: 0.324663\n",
            "train:  40% 200/500 [03:40<05:24,  1.08s/it]Train Epoch: 72 [200/500 (40%)] Loss: 0.338392\n",
            "train:  50% 250/500 [04:35<04:32,  1.09s/it]Train Epoch: 72 [250/500 (50%)] Loss: 0.413382\n",
            "train:  60% 300/500 [05:31<03:35,  1.08s/it]Train Epoch: 72 [300/500 (60%)] Loss: 0.308704\n",
            "train:  70% 350/500 [06:27<02:47,  1.12s/it]Train Epoch: 72 [350/500 (70%)] Loss: 0.347971\n",
            "train:  80% 400/500 [07:23<01:55,  1.15s/it]Train Epoch: 72 [400/500 (80%)] Loss: 0.318104\n",
            "train:  90% 450/500 [08:18<00:55,  1.10s/it]Train Epoch: 72 [450/500 (90%)] Loss: 0.421531\n",
            "train: 100% 500/500 [09:14<00:00,  1.11s/it]Train Epoch: 72 [500/500 (100%)] Loss: 0.357834\n",
            "train: 100% 500/500 [09:16<00:00,  1.11s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.38it/s]\n",
            "    epoch          : 72\n",
            "    loss           : 0.36476545572280883\n",
            "    grad norm      : 1.0445735704898835\n",
            "    WER (argmax)   : 0.2924120529648451\n",
            "    CER (argmax)   : 0.09960045899313519\n",
            "    val_loss       : 0.5814216885811243\n",
            "    val_WER (argmax): 0.3943102284668065\n",
            "    val_CER (argmax): 0.15042857095366372\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 73 [0/500 (0%)] Loss: 0.350346\n",
            "train:  10% 50/500 [00:55<08:18,  1.11s/it]Train Epoch: 73 [50/500 (10%)] Loss: 0.415973\n",
            "train:  20% 100/500 [01:51<07:29,  1.12s/it]Train Epoch: 73 [100/500 (20%)] Loss: 0.319584\n",
            "train:  30% 150/500 [02:48<06:28,  1.11s/it]Train Epoch: 73 [150/500 (30%)] Loss: 0.447551\n",
            "train:  40% 200/500 [03:44<05:27,  1.09s/it]Train Epoch: 73 [200/500 (40%)] Loss: 0.431893\n",
            "train:  50% 250/500 [04:41<04:39,  1.12s/it]Train Epoch: 73 [250/500 (50%)] Loss: 0.368555\n",
            "train:  60% 300/500 [05:37<03:37,  1.09s/it]Train Epoch: 73 [300/500 (60%)] Loss: 0.341354\n",
            "train:  70% 350/500 [06:34<02:50,  1.13s/it]Train Epoch: 73 [350/500 (70%)] Loss: 0.407510\n",
            "train:  80% 400/500 [07:30<01:52,  1.12s/it]Train Epoch: 73 [400/500 (80%)] Loss: 0.352008\n",
            "train:  90% 450/500 [08:27<00:55,  1.11s/it]Train Epoch: 73 [450/500 (90%)] Loss: 0.355615\n",
            "train: 100% 500/500 [09:25<00:00,  1.12s/it]Train Epoch: 73 [500/500 (100%)] Loss: 0.424447\n",
            "train: 100% 500/500 [09:27<00:00,  1.13s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.29it/s]\n",
            "    epoch          : 73\n",
            "    loss           : 0.3587936109304428\n",
            "    grad norm      : 0.9839643096923828\n",
            "    WER (argmax)   : 0.28699612346821857\n",
            "    CER (argmax)   : 0.09655116883536857\n",
            "    val_loss       : 0.5968988587458929\n",
            "    val_WER (argmax): 0.39487038837920635\n",
            "    val_CER (argmax): 0.1515039263349457\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 74 [0/500 (0%)] Loss: 0.371934\n",
            "train:  10% 50/500 [00:57<08:24,  1.12s/it]Train Epoch: 74 [50/500 (10%)] Loss: 0.401555\n",
            "train:  20% 100/500 [01:54<07:43,  1.16s/it]Train Epoch: 74 [100/500 (20%)] Loss: 0.349635\n",
            "train:  30% 150/500 [02:52<06:32,  1.12s/it]Train Epoch: 74 [150/500 (30%)] Loss: 0.338011\n",
            "train:  40% 200/500 [03:50<05:53,  1.18s/it]Train Epoch: 74 [200/500 (40%)] Loss: 0.355762\n",
            "train:  50% 250/500 [04:47<04:39,  1.12s/it]Train Epoch: 74 [250/500 (50%)] Loss: 0.333985\n",
            "train:  60% 300/500 [05:44<03:46,  1.13s/it]Train Epoch: 74 [300/500 (60%)] Loss: 0.335237\n",
            "train:  70% 350/500 [06:42<02:48,  1.12s/it]Train Epoch: 74 [350/500 (70%)] Loss: 0.343578\n",
            "train:  80% 400/500 [07:39<01:53,  1.13s/it]Train Epoch: 74 [400/500 (80%)] Loss: 0.384424\n",
            "train:  90% 450/500 [08:37<00:58,  1.17s/it]Train Epoch: 74 [450/500 (90%)] Loss: 0.383153\n",
            "train: 100% 500/500 [09:34<00:00,  1.12s/it]Train Epoch: 74 [500/500 (100%)] Loss: 0.357077\n",
            "train: 100% 500/500 [09:36<00:00,  1.15s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.29it/s]\n",
            "    epoch          : 74\n",
            "    loss           : 0.3685401147603989\n",
            "    grad norm      : 1.031737323999405\n",
            "    WER (argmax)   : 0.29524596533209346\n",
            "    CER (argmax)   : 0.1001300915370359\n",
            "    val_loss       : 0.5625047030357214\n",
            "    val_WER (argmax): 0.3849939436398294\n",
            "    val_CER (argmax): 0.14559420560081635\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 75 [0/500 (0%)] Loss: 0.351506\n",
            "train:  10% 50/500 [00:58<08:29,  1.13s/it]Train Epoch: 75 [50/500 (10%)] Loss: 0.333960\n",
            "train:  20% 100/500 [01:56<07:52,  1.18s/it]Train Epoch: 75 [100/500 (20%)] Loss: 0.318782\n",
            "train:  30% 150/500 [02:54<06:42,  1.15s/it]Train Epoch: 75 [150/500 (30%)] Loss: 0.376415\n",
            "train:  40% 200/500 [03:52<05:43,  1.15s/it]Train Epoch: 75 [200/500 (40%)] Loss: 0.385776\n",
            "train:  50% 250/500 [04:50<04:45,  1.14s/it]Train Epoch: 75 [250/500 (50%)] Loss: 0.388217\n",
            "train:  60% 300/500 [05:49<03:50,  1.15s/it]Train Epoch: 75 [300/500 (60%)] Loss: 0.324685\n",
            "train:  70% 350/500 [06:47<02:51,  1.14s/it]Train Epoch: 75 [350/500 (70%)] Loss: 0.390704\n",
            "train:  80% 400/500 [07:46<01:56,  1.17s/it]Train Epoch: 75 [400/500 (80%)] Loss: 0.325779\n",
            "train:  90% 450/500 [08:44<00:56,  1.14s/it]Train Epoch: 75 [450/500 (90%)] Loss: 0.385320\n",
            "train: 100% 500/500 [09:44<00:00,  1.16s/it]Train Epoch: 75 [500/500 (100%)] Loss: 0.265633\n",
            "train: 100% 500/500 [09:46<00:00,  1.17s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.25it/s]\n",
            "    epoch          : 75\n",
            "    loss           : 0.368760883808136\n",
            "    grad norm      : 1.0198728549480438\n",
            "    WER (argmax)   : 0.28938440778712704\n",
            "    CER (argmax)   : 0.09881057865241143\n",
            "    val_loss       : 0.5551165437851197\n",
            "    val_WER (argmax): 0.37866435150797517\n",
            "    val_CER (argmax): 0.14308099709063785\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 76 [0/500 (0%)] Loss: 0.368759\n",
            "train:  10% 50/500 [00:59<08:44,  1.17s/it]Train Epoch: 76 [50/500 (10%)] Loss: 0.435587\n",
            "train:  20% 100/500 [01:59<07:54,  1.19s/it]Train Epoch: 76 [100/500 (20%)] Loss: 0.378865\n",
            "train:  30% 150/500 [02:58<06:51,  1.17s/it]Train Epoch: 76 [150/500 (30%)] Loss: 0.313330\n",
            "train:  40% 200/500 [03:58<05:43,  1.14s/it]Train Epoch: 76 [200/500 (40%)] Loss: 0.321994\n",
            "train:  50% 250/500 [04:58<05:02,  1.21s/it]Train Epoch: 76 [250/500 (50%)] Loss: 0.328125\n",
            "train:  60% 300/500 [05:57<03:51,  1.16s/it]Train Epoch: 76 [300/500 (60%)] Loss: 0.322238\n",
            "train:  70% 350/500 [06:57<03:00,  1.20s/it]Train Epoch: 76 [350/500 (70%)] Loss: 0.339167\n",
            "train:  80% 400/500 [07:57<02:01,  1.22s/it]Train Epoch: 76 [400/500 (80%)] Loss: 0.349322\n",
            "train:  90% 450/500 [08:56<00:59,  1.18s/it]Train Epoch: 76 [450/500 (90%)] Loss: 0.341445\n",
            "train: 100% 500/500 [09:56<00:00,  1.17s/it]Train Epoch: 76 [500/500 (100%)] Loss: 0.393325\n",
            "train: 100% 500/500 [09:58<00:00,  1.20s/it]\n",
            "val: 100% 78/78 [00:15<00:00,  5.18it/s]\n",
            "    epoch          : 76\n",
            "    loss           : 0.34705387413501737\n",
            "    grad norm      : 1.030926569700241\n",
            "    WER (argmax)   : 0.2762323329175241\n",
            "    CER (argmax)   : 0.09312440436003039\n",
            "    val_loss       : 0.5826240181922913\n",
            "    val_WER (argmax): 0.382790564785443\n",
            "    val_CER (argmax): 0.14551059222686577\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 77 [0/500 (0%)] Loss: 0.448705\n",
            "train:  10% 50/500 [00:59<08:57,  1.19s/it]Train Epoch: 77 [50/500 (10%)] Loss: 0.365222\n",
            "train:  20% 100/500 [02:00<08:01,  1.20s/it]Train Epoch: 77 [100/500 (20%)] Loss: 0.317511\n",
            "train:  30% 150/500 [03:00<06:52,  1.18s/it]Train Epoch: 77 [150/500 (30%)] Loss: 0.333861\n",
            "train:  40% 200/500 [04:00<06:13,  1.24s/it]Train Epoch: 77 [200/500 (40%)] Loss: 0.389879\n",
            "train:  50% 250/500 [05:01<04:58,  1.19s/it]Train Epoch: 77 [250/500 (50%)] Loss: 0.360059\n",
            "train:  60% 300/500 [06:02<04:02,  1.21s/it]Train Epoch: 77 [300/500 (60%)] Loss: 0.324054\n",
            "train:  70% 350/500 [07:02<02:59,  1.20s/it]Train Epoch: 77 [350/500 (70%)] Loss: 0.341168\n",
            "train:  80% 400/500 [08:02<02:01,  1.21s/it]Train Epoch: 77 [400/500 (80%)] Loss: 0.329771\n",
            "train:  90% 450/500 [09:03<01:00,  1.21s/it]Train Epoch: 77 [450/500 (90%)] Loss: 0.323614\n",
            "train: 100% 500/500 [10:04<00:00,  1.19s/it]Train Epoch: 77 [500/500 (100%)] Loss: 0.297311\n",
            "train: 100% 500/500 [10:06<00:00,  1.21s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.21it/s]\n",
            "    epoch          : 77\n",
            "    loss           : 0.34018416225910186\n",
            "    grad norm      : 0.9628446042537689\n",
            "    WER (argmax)   : 0.2785107658741973\n",
            "    CER (argmax)   : 0.09334662650480259\n",
            "    val_loss       : 0.5559152720066217\n",
            "    val_WER (argmax): 0.37515677542473813\n",
            "    val_CER (argmax): 0.1420190714124463\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 78 [0/500 (0%)] Loss: 0.374955\n",
            "train:  10% 50/500 [01:01<08:54,  1.19s/it]Train Epoch: 78 [50/500 (10%)] Loss: 0.469399\n",
            "train:  20% 100/500 [02:02<07:51,  1.18s/it]Train Epoch: 78 [100/500 (20%)] Loss: 0.407680\n",
            "train:  30% 150/500 [03:03<06:57,  1.19s/it]Train Epoch: 78 [150/500 (30%)] Loss: 0.351377\n",
            "train:  40% 200/500 [04:05<06:03,  1.21s/it]Train Epoch: 78 [200/500 (40%)] Loss: 0.312901\n",
            "train:  50% 250/500 [05:06<05:01,  1.21s/it]Train Epoch: 78 [250/500 (50%)] Loss: 0.404728\n",
            "train:  60% 300/500 [06:07<03:55,  1.18s/it]Train Epoch: 78 [300/500 (60%)] Loss: 0.293026\n",
            "train:  70% 350/500 [07:08<03:06,  1.25s/it]Train Epoch: 78 [350/500 (70%)] Loss: 0.434005\n",
            "train:  80% 400/500 [08:10<02:00,  1.20s/it]Train Epoch: 78 [400/500 (80%)] Loss: 0.345209\n",
            "train:  90% 450/500 [09:12<00:59,  1.20s/it]Train Epoch: 78 [450/500 (90%)] Loss: 0.431660\n",
            "train: 100% 500/500 [10:13<00:00,  1.20s/it]Train Epoch: 78 [500/500 (100%)] Loss: 0.332538\n",
            "train: 100% 500/500 [10:15<00:00,  1.23s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.30it/s]\n",
            "    epoch          : 78\n",
            "    loss           : 0.3563103085756302\n",
            "    grad norm      : 1.0832253122329711\n",
            "    WER (argmax)   : 0.28353455918290427\n",
            "    CER (argmax)   : 0.09680905140198033\n",
            "    val_loss       : 0.5695055386958978\n",
            "    val_WER (argmax): 0.3764513534306199\n",
            "    val_CER (argmax): 0.14276063917883017\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 79 [0/500 (0%)] Loss: 0.355158\n",
            "train:  10% 50/500 [01:01<09:11,  1.23s/it]Train Epoch: 79 [50/500 (10%)] Loss: 0.414859\n",
            "train:  20% 100/500 [02:03<08:11,  1.23s/it]Train Epoch: 79 [100/500 (20%)] Loss: 0.309071\n",
            "train:  30% 150/500 [03:05<07:00,  1.20s/it]Train Epoch: 79 [150/500 (30%)] Loss: 0.287737\n",
            "train:  40% 200/500 [04:07<06:06,  1.22s/it]Train Epoch: 79 [200/500 (40%)] Loss: 0.330770\n",
            "train:  50% 250/500 [05:09<05:05,  1.22s/it]Train Epoch: 79 [250/500 (50%)] Loss: 0.377150\n",
            "train:  60% 300/500 [06:12<04:06,  1.23s/it]Train Epoch: 79 [300/500 (60%)] Loss: 0.454784\n",
            "train:  70% 350/500 [07:14<03:03,  1.22s/it]Train Epoch: 79 [350/500 (70%)] Loss: 0.374830\n",
            "train:  80% 400/500 [08:16<02:03,  1.24s/it]Train Epoch: 79 [400/500 (80%)] Loss: 0.410895\n",
            "train:  90% 450/500 [09:19<01:00,  1.21s/it]Train Epoch: 79 [450/500 (90%)] Loss: 0.393144\n",
            "train: 100% 500/500 [10:21<00:00,  1.23s/it]Train Epoch: 79 [500/500 (100%)] Loss: 0.248778\n",
            "train: 100% 500/500 [10:23<00:00,  1.25s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.30it/s]\n",
            "    epoch          : 79\n",
            "    loss           : 0.3375651651620865\n",
            "    grad norm      : 0.9903999245166779\n",
            "    WER (argmax)   : 0.27356435067284485\n",
            "    CER (argmax)   : 0.09165201198829456\n",
            "    val_loss       : 0.5557792702546487\n",
            "    val_WER (argmax): 0.3746666044330675\n",
            "    val_CER (argmax): 0.14293493779341368\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 80 [0/500 (0%)] Loss: 0.315784\n",
            "train:  10% 50/500 [01:02<09:21,  1.25s/it]Train Epoch: 80 [50/500 (10%)] Loss: 0.322122\n",
            "train:  20% 100/500 [02:06<08:15,  1.24s/it]Train Epoch: 80 [100/500 (20%)] Loss: 0.362849\n",
            "train:  30% 150/500 [03:08<07:10,  1.23s/it]Train Epoch: 80 [150/500 (30%)] Loss: 0.378652\n",
            "train:  40% 200/500 [04:12<06:12,  1.24s/it]Train Epoch: 80 [200/500 (40%)] Loss: 0.311402\n",
            "train:  50% 250/500 [05:14<05:03,  1.21s/it]Train Epoch: 80 [250/500 (50%)] Loss: 0.294618\n",
            "train:  60% 300/500 [06:17<04:09,  1.25s/it]Train Epoch: 80 [300/500 (60%)] Loss: 0.313638\n",
            "train:  70% 350/500 [07:21<03:07,  1.25s/it]Train Epoch: 80 [350/500 (70%)] Loss: 0.322849\n",
            "train:  80% 400/500 [08:24<02:04,  1.24s/it]Train Epoch: 80 [400/500 (80%)] Loss: 0.303940\n",
            "train:  90% 450/500 [09:28<01:01,  1.23s/it]Train Epoch: 80 [450/500 (90%)] Loss: 0.312094\n",
            "train: 100% 500/500 [10:31<00:00,  1.23s/it]Train Epoch: 80 [500/500 (100%)] Loss: 0.275369\n",
            "train: 100% 500/500 [10:33<00:00,  1.27s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.28it/s]\n",
            "    epoch          : 80\n",
            "    loss           : 0.35062060654163363\n",
            "    grad norm      : 0.9243364226818085\n",
            "    WER (argmax)   : 0.2782506181090369\n",
            "    CER (argmax)   : 0.09401608042452962\n",
            "    val_loss       : 0.5591586163410773\n",
            "    val_WER (argmax): 0.3656222060314762\n",
            "    val_CER (argmax): 0.13896916134607085\n",
            "Saving checkpoint: saved/models/conformer/1028_152312/checkpoint-epoch80.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 81 [0/500 (0%)] Loss: 0.365619\n",
            "train:  10% 50/500 [01:03<09:22,  1.25s/it]Train Epoch: 81 [50/500 (10%)] Loss: 0.312533\n",
            "train:  20% 100/500 [02:06<08:16,  1.24s/it]Train Epoch: 81 [100/500 (20%)] Loss: 0.414577\n",
            "train:  30% 150/500 [03:10<07:18,  1.25s/it]Train Epoch: 81 [150/500 (30%)] Loss: 0.286609\n",
            "train:  40% 200/500 [04:14<06:14,  1.25s/it]Train Epoch: 81 [200/500 (40%)] Loss: 0.362853\n",
            "train:  50% 250/500 [05:18<05:06,  1.23s/it]Train Epoch: 81 [250/500 (50%)] Loss: 0.310225\n",
            "train:  60% 300/500 [06:23<04:12,  1.26s/it]Train Epoch: 81 [300/500 (60%)] Loss: 0.327416\n",
            "train:  70% 350/500 [07:28<03:09,  1.26s/it]Train Epoch: 81 [350/500 (70%)] Loss: 0.340343\n",
            "train:  80% 400/500 [08:32<02:05,  1.26s/it]Train Epoch: 81 [400/500 (80%)] Loss: 0.348279\n",
            "train:  90% 450/500 [09:36<01:02,  1.25s/it]Train Epoch: 81 [450/500 (90%)] Loss: 0.413022\n",
            "train: 100% 500/500 [10:40<00:00,  1.28s/it]Train Epoch: 81 [500/500 (100%)] Loss: 0.415923\n",
            "train: 100% 500/500 [10:42<00:00,  1.29s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.25it/s]\n",
            "    epoch          : 81\n",
            "    loss           : 0.33957831919193265\n",
            "    grad norm      : 0.9981117951869964\n",
            "    WER (argmax)   : 0.27669519256237957\n",
            "    CER (argmax)   : 0.09187889806031721\n",
            "    val_loss       : 0.5579839635353822\n",
            "    val_WER (argmax): 0.37433734720283185\n",
            "    val_CER (argmax): 0.14134934831623644\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 82 [0/500 (0%)] Loss: 0.274177\n",
            "train:  10% 50/500 [01:05<09:32,  1.27s/it]Train Epoch: 82 [50/500 (10%)] Loss: 0.316096\n",
            "train:  20% 100/500 [02:11<08:43,  1.31s/it]Train Epoch: 82 [100/500 (20%)] Loss: 0.361931\n",
            "train:  30% 150/500 [03:16<07:15,  1.24s/it]Train Epoch: 82 [150/500 (30%)] Loss: 0.308782\n",
            "train:  40% 200/500 [04:21<06:21,  1.27s/it]Train Epoch: 82 [200/500 (40%)] Loss: 0.355734\n",
            "train:  50% 250/500 [05:26<05:19,  1.28s/it]Train Epoch: 82 [250/500 (50%)] Loss: 0.304375\n",
            "train:  60% 300/500 [06:31<04:18,  1.29s/it]Train Epoch: 82 [300/500 (60%)] Loss: 0.363379\n",
            "train:  70% 350/500 [07:37<03:12,  1.28s/it]Train Epoch: 82 [350/500 (70%)] Loss: 0.303117\n",
            "train:  80% 400/500 [08:42<02:09,  1.30s/it]Train Epoch: 82 [400/500 (80%)] Loss: 0.307006\n",
            "train:  90% 450/500 [09:48<01:03,  1.28s/it]Train Epoch: 82 [450/500 (90%)] Loss: 0.346571\n",
            "train: 100% 500/500 [10:54<00:00,  1.30s/it]Train Epoch: 82 [500/500 (100%)] Loss: 0.292975\n",
            "train: 100% 500/500 [10:56<00:00,  1.31s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.38it/s]\n",
            "    epoch          : 82\n",
            "    loss           : 0.33463922679424285\n",
            "    grad norm      : 0.9987579441070557\n",
            "    WER (argmax)   : 0.27582772567783065\n",
            "    CER (argmax)   : 0.09323738473017824\n",
            "    val_loss       : 0.5485614595504907\n",
            "    val_WER (argmax): 0.36974255396222105\n",
            "    val_CER (argmax): 0.13977251148997702\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 83 [0/500 (0%)] Loss: 0.377418\n",
            "train:  10% 50/500 [01:06<09:57,  1.33s/it]Train Epoch: 83 [50/500 (10%)] Loss: 0.379736\n",
            "train:  20% 100/500 [02:12<08:38,  1.30s/it]Train Epoch: 83 [100/500 (20%)] Loss: 0.381050\n",
            "train:  30% 150/500 [03:18<07:40,  1.31s/it]Train Epoch: 83 [150/500 (30%)] Loss: 0.366672\n",
            "train:  40% 200/500 [04:24<06:24,  1.28s/it]Train Epoch: 83 [200/500 (40%)] Loss: 0.407771\n",
            "train:  50% 250/500 [05:31<05:28,  1.31s/it]Train Epoch: 83 [250/500 (50%)] Loss: 0.389581\n",
            "train:  60% 300/500 [06:37<04:16,  1.28s/it]Train Epoch: 83 [300/500 (60%)] Loss: 0.443809\n",
            "train:  70% 350/500 [07:43<03:15,  1.30s/it]Train Epoch: 83 [350/500 (70%)] Loss: 0.278600\n",
            "train:  80% 400/500 [08:49<02:08,  1.29s/it]Train Epoch: 83 [400/500 (80%)] Loss: 0.329451\n",
            "train:  90% 450/500 [09:55<01:05,  1.30s/it]Train Epoch: 83 [450/500 (90%)] Loss: 0.336981\n",
            "train: 100% 500/500 [11:00<00:00,  1.28s/it]Train Epoch: 83 [500/500 (100%)] Loss: 0.373526\n",
            "train: 100% 500/500 [11:03<00:00,  1.33s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.32it/s]\n",
            "    epoch          : 83\n",
            "    loss           : 0.3393655562400818\n",
            "    grad norm      : 1.0137544751167298\n",
            "    WER (argmax)   : 0.2761890553743838\n",
            "    CER (argmax)   : 0.09331465269801072\n",
            "    val_loss       : 0.553713708733901\n",
            "    val_WER (argmax): 0.3627836713868273\n",
            "    val_CER (argmax): 0.13661538183020297\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 84 [0/500 (0%)] Loss: 0.336478\n",
            "train:  10% 50/500 [01:06<09:46,  1.30s/it]Train Epoch: 84 [50/500 (10%)] Loss: 0.354269\n",
            "train:  20% 100/500 [02:13<08:41,  1.30s/it]Train Epoch: 84 [100/500 (20%)] Loss: 0.367659\n",
            "train:  30% 150/500 [03:20<07:48,  1.34s/it]Train Epoch: 84 [150/500 (30%)] Loss: 0.403796\n",
            "train:  40% 200/500 [04:27<06:30,  1.30s/it]Train Epoch: 84 [200/500 (40%)] Loss: 0.362003\n",
            "train:  50% 250/500 [05:33<05:25,  1.30s/it]Train Epoch: 84 [250/500 (50%)] Loss: 0.360988\n",
            "train:  60% 300/500 [06:41<04:27,  1.34s/it]Train Epoch: 84 [300/500 (60%)] Loss: 0.357188\n",
            "train:  70% 350/500 [07:48<03:15,  1.31s/it]Train Epoch: 84 [350/500 (70%)] Loss: 0.318360\n",
            "train:  80% 400/500 [08:54<02:10,  1.31s/it]Train Epoch: 84 [400/500 (80%)] Loss: 0.434263\n",
            "train:  90% 450/500 [10:02<01:06,  1.34s/it]Train Epoch: 84 [450/500 (90%)] Loss: 0.322176\n",
            "train: 100% 500/500 [11:10<00:00,  1.31s/it]Train Epoch: 84 [500/500 (100%)] Loss: 0.353266\n",
            "train: 100% 500/500 [11:12<00:00,  1.34s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.23it/s]\n",
            "    epoch          : 84\n",
            "    loss           : 0.33735592365264894\n",
            "    grad norm      : 0.9852006363868714\n",
            "    WER (argmax)   : 0.2706518670873576\n",
            "    CER (argmax)   : 0.09120983992781782\n",
            "    val_loss       : 0.5609343965084125\n",
            "    val_WER (argmax): 0.36473002062516874\n",
            "    val_CER (argmax): 0.1390409766275217\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 85 [0/500 (0%)] Loss: 0.320924\n",
            "train:  10% 50/500 [01:07<10:08,  1.35s/it]Train Epoch: 85 [50/500 (10%)] Loss: 0.355620\n",
            "train:  20% 100/500 [02:15<09:21,  1.40s/it]Train Epoch: 85 [100/500 (20%)] Loss: 0.363903\n",
            "train:  30% 150/500 [03:23<07:47,  1.33s/it]Train Epoch: 85 [150/500 (30%)] Loss: 0.346010\n",
            "train:  40% 200/500 [04:31<06:44,  1.35s/it]Train Epoch: 85 [200/500 (40%)] Loss: 0.289126\n",
            "train:  50% 250/500 [05:39<05:30,  1.32s/it]Train Epoch: 85 [250/500 (50%)] Loss: 0.332999\n",
            "train:  60% 300/500 [06:47<04:39,  1.40s/it]Train Epoch: 85 [300/500 (60%)] Loss: 0.418644\n",
            "train:  70% 350/500 [07:56<03:19,  1.33s/it]Train Epoch: 85 [350/500 (70%)] Loss: 0.351994\n",
            "train:  80% 400/500 [09:04<02:17,  1.38s/it]Train Epoch: 85 [400/500 (80%)] Loss: 0.334860\n",
            "train:  90% 450/500 [10:12<01:07,  1.36s/it]Train Epoch: 85 [450/500 (90%)] Loss: 0.319591\n",
            "train: 100% 500/500 [11:21<00:00,  1.35s/it]Train Epoch: 85 [500/500 (100%)] Loss: 0.286123\n",
            "train: 100% 500/500 [11:23<00:00,  1.37s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.32it/s]\n",
            "    epoch          : 85\n",
            "    loss           : 0.32883147299289706\n",
            "    grad norm      : 0.9865865468978882\n",
            "    WER (argmax)   : 0.26661518315556065\n",
            "    CER (argmax)   : 0.08870381749797739\n",
            "    val_loss       : 0.5515628204895899\n",
            "    val_WER (argmax): 0.3711977058245339\n",
            "    val_CER (argmax): 0.1410187562032624\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 86 [0/500 (0%)] Loss: 0.334916\n",
            "train:  10% 50/500 [01:08<10:10,  1.36s/it]Train Epoch: 86 [50/500 (10%)] Loss: 0.287564\n",
            "train:  20% 100/500 [02:16<09:15,  1.39s/it]Train Epoch: 86 [100/500 (20%)] Loss: 0.284453\n",
            "train:  30% 150/500 [03:24<07:49,  1.34s/it]Train Epoch: 86 [150/500 (30%)] Loss: 0.319512\n",
            "train:  40% 200/500 [04:33<06:47,  1.36s/it]Train Epoch: 86 [200/500 (40%)] Loss: 0.291890\n",
            "train:  50% 250/500 [05:41<05:41,  1.37s/it]Train Epoch: 86 [250/500 (50%)] Loss: 0.298310\n",
            "train:  60% 300/500 [06:50<04:33,  1.37s/it]Train Epoch: 86 [300/500 (60%)] Loss: 0.397764\n",
            "train:  70% 350/500 [08:00<03:24,  1.36s/it]Train Epoch: 86 [350/500 (70%)] Loss: 0.275514\n",
            "train:  80% 400/500 [09:09<02:16,  1.36s/it]Train Epoch: 86 [400/500 (80%)] Loss: 0.283837\n",
            "train:  90% 450/500 [10:18<01:08,  1.36s/it]Train Epoch: 86 [450/500 (90%)] Loss: 0.255329\n",
            "train: 100% 500/500 [11:27<00:00,  1.35s/it]Train Epoch: 86 [500/500 (100%)] Loss: 0.268964\n",
            "train: 100% 500/500 [11:30<00:00,  1.38s/it]\n",
            "val: 100% 78/78 [00:14<00:00,  5.29it/s]\n",
            "    epoch          : 86\n",
            "    loss           : 0.32493369102478026\n",
            "    grad norm      : 0.9702977848052978\n",
            "    WER (argmax)   : 0.2614267385093328\n",
            "    CER (argmax)   : 0.08767014047188586\n",
            "    val_loss       : 0.5501891504495572\n",
            "    val_WER (argmax): 0.3640482054708942\n",
            "    val_CER (argmax): 0.1382055655790687\n",
            "train:   0% 0/500 [00:00<?, ?it/s]Train Epoch: 87 [0/500 (0%)] Loss: 0.270818\n",
            "train:   5% 23/500 [00:32<11:04,  1.39s/it]\n",
            "Saving model on keyboard interrupt\n",
            "Saving checkpoint: saved/models/conformer/1028_152312/checkpoint-epoch87.pth ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
            "    c.send((key, os.getpid()))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/hw_asr/train.py\", line 107, in <module>\n",
            "    main(config)\n",
            "  File \"/content/hw_asr/train.py\", line 71, in main\n",
            "    trainer.train()\n",
            "  File \"/content/hw_asr/hw_asr/base/base_trainer.py\", line 73, in train\n",
            "    raise e\n",
            "  File \"/content/hw_asr/hw_asr/base/base_trainer.py\", line 69, in train\n",
            "    self._train_process()\n",
            "  File \"/content/hw_asr/hw_asr/base/base_trainer.py\", line 82, in _train_process\n",
            "    result = self._train_epoch(epoch)\n",
            "  File \"/content/hw_asr/hw_asr/trainer/trainer.py\", line 90, in _train_epoch\n",
            "    for batch_idx, batch in enumerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1182, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/content/hw_asr/hw_asr/utils/util.py\", line 33, in inf_loop\n",
            "    yield from loader\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1294, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  CER (argmax)_train ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    CER (argmax)_val ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  WER (argmax)_train ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    WER (argmax)_val ‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch_ ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch_val ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     grad norm_train ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning rate_train ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss_train ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_val ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: steps_per_sec_train ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   steps_per_sec_val ‚ñÇ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  CER (argmax)_train 0.0699\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    CER (argmax)_val 0.13821\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  WER (argmax)_train 0.2214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    WER (argmax)_val 0.36405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch_ 56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch_val 87\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     grad norm_train 0.80322\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning rate_train 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss_train 0.27082\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_val 0.55019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: steps_per_sec_train 0.37472\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   steps_per_sec_val 0.06353\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mexpert-yogurt-65\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/karimdzan/asr_project/runs/wqc6wfns\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ô∏è‚ö° View job at \u001b[34m\u001b[4mhttps://wandb.ai/karimdzan/asr_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDUxOTMzMQ==/version_details/v9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 746 media file(s), 375 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231028_152315-wqc6wfns/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!cd hw_asr && python3 train.py -r /content/hw_asr/saved/models/conformer/fine/model_best_fine.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search with LM"
      ],
      "metadata": {
        "id": "HbEzJrFlHxU8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz9YsvkDNGA4"
      },
      "outputs": [],
      "source": [
        "!wget https://www.openslr.org/resources/11/3-gram.arpa.gz --no-check-certificate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4N7H-c1lP2ME"
      },
      "outputs": [],
      "source": [
        "!gzip -d 3-gram.arpa.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqGxYAc8OP-N",
        "outputId": "6846c69e-1706-4dba-ac8e-2c3ada9a2caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "Loading the LM will be faster if you build a binary file.\n",
            "Reading /content/hw_asr/3-gram.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Using arpa instead of binary LM file, decoder instantiation might be slow.\n",
            "Alphabet determined to be of regular style.\n",
            "Conformer(\n",
            "  (layers): ModuleList(\n",
            "    (0-11): 12 x ConformerBlock(\n",
            "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff1): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (2): Swish()\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "          (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (5): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (attn): MultiHeadedSelfAttentionModule(\n",
            "        (positional_encoding): PositionalEncoding()\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention): RelativeMultiHeadAttention(\n",
            "          (query_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (key_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (value_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (pos_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (conv): ConformerConvModule(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Rearrange('b n c -> b c n')\n",
            "          (2): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
            "          (3): GLU()\n",
            "          (4): DepthWiseConv1d(\n",
            "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), groups=256)\n",
            "          )\n",
            "          (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (6): Swish()\n",
            "          (7): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "          (8): Rearrange('b c n -> b n c')\n",
            "          (9): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ff2): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (2): Swish()\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "          (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (5): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_subsample): Conv2dSubsampling(\n",
            "    (sequential): Sequential(\n",
            "      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (input_proj): Sequential(\n",
            "    (0): Linear(in_features=3968, out_features=128, bias=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=28, bias=False)\n",
            ")\n",
            "Loading checkpoint: /content/hw_asr/saved/models/conformer/1028_152312/model_best.pth ...\n",
            "100% 41/41 [03:41<00:00,  5.40s/it]\n",
            "WER_argmax: 0.2806\n",
            "WER_beam_search: 0.1642\n",
            "CER_argmax: 0.0903\n",
            "CER_beam_search: 0.0697\n"
          ]
        }
      ],
      "source": [
        "!cd hw_asr && python3 test.py \\\n",
        "    --config hw_asr/configs/conformer_evaluate_test_clean.json \\\n",
        "    --resume /content/hw_asr/saved/models/conformer/1028_152312/model_best.pth \\\n",
        "    --batch-size 64 \\\n",
        "    --jobs 4 \\\n",
        "    --beam-size 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMHWFCBsOmdV",
        "outputId": "db0e2fca-7454-4d80-85f9-c7a6a1538431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "Loading the LM will be faster if you build a binary file.\n",
            "Reading /content/hw_asr/3-gram.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Using arpa instead of binary LM file, decoder instantiation might be slow.\n",
            "Alphabet determined to be of regular style.\n",
            "Conformer(\n",
            "  (layers): ModuleList(\n",
            "    (0-11): 12 x ConformerBlock(\n",
            "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff1): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (2): Swish()\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "          (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (5): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (attn): MultiHeadedSelfAttentionModule(\n",
            "        (positional_encoding): PositionalEncoding()\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention): RelativeMultiHeadAttention(\n",
            "          (query_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (key_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (value_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (pos_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (conv): ConformerConvModule(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Rearrange('b n c -> b c n')\n",
            "          (2): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
            "          (3): GLU()\n",
            "          (4): DepthWiseConv1d(\n",
            "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), groups=256)\n",
            "          )\n",
            "          (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (6): Swish()\n",
            "          (7): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "          (8): Rearrange('b c n -> b n c')\n",
            "          (9): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ff2): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (2): Swish()\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "          (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (5): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_subsample): Conv2dSubsampling(\n",
            "    (sequential): Sequential(\n",
            "      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (input_proj): Sequential(\n",
            "    (0): Linear(in_features=3968, out_features=128, bias=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=28, bias=False)\n",
            ")\n",
            "Loading checkpoint: /content/hw_asr/saved/models/conformer/1028_152312/model_best.pth ...\n",
            "100% 46/46 [06:44<00:00,  8.80s/it]\n",
            "WER_argmax: 0.4557\n",
            "WER_beam_search: 0.3277\n",
            "CER_argmax: 0.1844\n",
            "CER_beam_search: 0.1650\n"
          ]
        }
      ],
      "source": [
        "!cd hw_asr && python3 test.py \\\n",
        "    --config hw_asr/configs/conformer_evaluate_test_other.json \\\n",
        "    --resume /content/hw_asr/saved/models/conformer/1028_152312/model_best.pth \\\n",
        "    --batch-size 64 \\\n",
        "    --jobs 4 \\\n",
        "    --beam-size 100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search without LM"
      ],
      "metadata": {
        "id": "WhDS_ljsHtDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd hw_asr && python3 test.py \\\n",
        "    --config hw_asr/configs/conformer_evaluate_test_clean.json \\\n",
        "    --resume /content/hw_asr/saved/models/conformer/1028_152312/model_best.pth \\\n",
        "    --batch-size 64 \\\n",
        "    --jobs 4 \\\n",
        "    --beam-size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr8-s4sdHriq",
        "outputId": "580abe55-f15b-418a-ce79-fb54c68d2956"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "Loading the LM will be faster if you build a binary file.\n",
            "Reading /content/hw_asr/3-gram.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Using arpa instead of binary LM file, decoder instantiation might be slow.\n",
            "Alphabet determined to be of regular style.\n",
            "Conformer(\n",
            "  (layers): ModuleList(\n",
            "    (0-11): 12 x ConformerBlock(\n",
            "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff1): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (2): Swish()\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "          (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (5): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (attn): MultiHeadedSelfAttentionModule(\n",
            "        (positional_encoding): PositionalEncoding()\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention): RelativeMultiHeadAttention(\n",
            "          (query_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (key_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (value_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (pos_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (conv): ConformerConvModule(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Rearrange('b n c -> b c n')\n",
            "          (2): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
            "          (3): GLU()\n",
            "          (4): DepthWiseConv1d(\n",
            "            (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), groups=256)\n",
            "          )\n",
            "          (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (6): Swish()\n",
            "          (7): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "          (8): Rearrange('b c n -> b n c')\n",
            "          (9): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ff2): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (2): Swish()\n",
            "          (3): Dropout(p=0.0, inplace=False)\n",
            "          (4): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (5): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_subsample): Conv2dSubsampling(\n",
            "    (sequential): Sequential(\n",
            "      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (input_proj): Sequential(\n",
            "    (0): Linear(in_features=3968, out_features=128, bias=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=28, bias=False)\n",
            ")\n",
            "Loading checkpoint: /content/hw_asr/saved/models/conformer/1028_152312/model_best.pth ...\n",
            "100% 41/41 [08:52<00:00, 12.99s/it]\n",
            "WER_argmax: 0.2803\n",
            "WER_beam_search: 0.2813\n",
            "CER_argmax: 0.0907\n",
            "CER_beam_search: 0.0934\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/hw_asr/test.py\", line 195, in <module>\n",
            "    main(config, args.output, args.beam_size)\n",
            "  File \"/content/hw_asr/test.py\", line 90, in main\n",
            "    json.dump(results, f, indent=2)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 179, in dump\n",
            "    for chunk in iterable:\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 429, in _iterencode\n",
            "    yield from _iterencode_list(o, _current_indent_level)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 325, in _iterencode_list\n",
            "    yield from chunks\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n",
            "    yield from chunks\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 325, in _iterencode_list\n",
            "    yield from chunks\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 325, in _iterencode_list\n",
            "    yield from chunks\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n",
            "    o = _default(o)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n",
            "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
            "TypeError: Object of type Tensor is not JSON serializable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd hw_asr && python3 test.py \\\n",
        "    --config hw_asr/configs/conformer_evaluate_test_other.json \\\n",
        "    --resume /content/hw_asr/saved/models/conformer/1028_152312/model_best.pth \\\n",
        "    --batch-size 64 \\\n",
        "    --jobs 4 \\\n",
        "    --beam-size 2"
      ],
      "metadata": {
        "id": "jEs3EOu3HsUf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}